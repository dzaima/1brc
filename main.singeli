include 'singeli-java/base'

def bulk = 32 # byte count for maximum-parallel ops
def exp_bulk = 16 # maximum name length to handle in fast path
def interleaved = 4 # number of semicolon searches to interleave
def bufmax = 1000 # TODO 2^n-proc_bulk
def takes0{x} = match(x) { {(32)} => 3 } # max number of semicolons fast path of search should handle in N bytes
def HASH = u32 # full hash
def HASHV = [4]HASH # buckets to search

def proc_bulk = 4 # number of records to handle in a processing iteration
def VIX = [proc_bulk]vptr_store
def ri = range{interleaved}

config playground = 0
def one = playground
(if (not playground) require{'header.h'})
def lpf1{...vs} = if (one) lprintf{...vs}
def  pf1{...vs} = if (one)  printf{...vs}
def lps1{...vs} = if (one) printstr{...vs}

# define "a +{i32} b" etc to reinterpret as the element, do the op, and cast back
local def retype_cond{C} = isprim{C}|veca{C}
local def retype_type{C, T} = if (veca{C}) C else re_el{C, T}
local def extend retype2{op} = {
  def op{C & retype_cond{C}} = {
    def c{T} = retype_type{C, T}
    def me{a:T,    b:c{T}} = T~~op{c{T}~~a, b}
    def me{a:c{T}, b:T   } = T~~op{a, c{T}~~b}
    def me{a:T,    b:T   } = T~~op{c{T}~~a, c{T}~~b}
  }
}
extend ({...f}=>each{retype2,f}){
  __add, __sub, __mul, __div, __mod,
  __eq, __ne, __gt, __ge, __lt, __le,
  __and, __or, __xor, __min, __max,
  __subs, __adds, v_eq, v_ne
}
local def extend retypeShift{op} = {
  def op{C & retype_cond{C}}{a:T, b} = T~~op{retype_type{C,T}~~a, b}
}
extend retypeShift{__shr}
extend retypeShift{__shl}
def l128_upper_u64{x:T & width{T}==128} = shuf{[2]u64, x, 1, 1}
def l128_upper_u64{x:T & width{T}==256} = shuf{[4]u64, x, 1, 1, 3, 3}


def iter_count{bulk} = (bufmax/(interleaved*takes0{bulk})) >> 0

def ly = buflayout{
  1, 128, 'pad',
  1, interleaved, 'bufsEnds',
  interleaved, bufmax+proc_bulk, 'bufs',
  1, bufmax*interleaved*2, 'faildata', # pairs of offset, temperature<<1 | long
  1, 128, 'pad',
}

def periter = bulk * interleaved * iter_count{bulk}
fn core_1brc_buf_elts() : ux = ly{'.size'}
fn core_1brc_periter() : ux = periter


# TODO could pack to i64,i32,i16,i16
def dt_sum = 0
def dt_num = 1
def dt_min = 2
def dt_max = 3

fn core_1brc(
  ident:i32,
  buf0:*vptr_store,
  
  hash_mask:HASH, # map length (power of two), minus 1
  
  # indexed by masked hash
  map_exp:*i8,    # chunks of exp_bulk
  map_hash:*HASH, # full hash
  map_data:*i64,  # chunks of 4: min, max, sum, count
  
  inp:*i8,
  inpOff:ux
) : void = @withbufs(object_arr) {
  def takes = takes0{bulk}
  def V = [bulk]i8
  
  def bufs = each{ly{'bufs'}{buf0, .}, ri}
  def failposS = ly{'faildata'}{buf0, 0}
  def failposC = failposS{'copy'}
  
  ########## SEMICOLON SEARCH ##########
  def ptrs = each{{i} => vptr{inp, inpOff + i*bulk}, ri}
  @for(i to iter_count{bulk}) {
    def ms0 = each{{ptr} => homMask{ptr{'load', V, 0} == V**59}, ptrs}
    
    def ms = each{{x} => {m:=x}, ms0}
    def adv_mask{buf, ptr, m, j, m0tmp} = {
      m2:= m & (m-1)
      buf{'store', j, ptr{'to_store', ctz{m}}} # TODO move to_store to a vectorized add at the end?
      m = m2
    }
    @for_const(j to takes) {
      each{{ptr, buf, m,m0} => adv_mask{buf, ptr, m, j, m0}, ptrs, bufs, ms,ms0}
    }
    
    each{{i, m0, m, buf, ptr} => {
      pop:= popc{m0}
      if (rare{pop > takes}) { # TODO move to an outer if that checks if any is over?
        buf{'bump', takes}
        do {
          adv_mask{buf, ptr, m, 0, m0}
          buf{'bump', 1}
        } while (m != 0)
      } else {
        buf{'bump', pop}
      }
      ptr{'bump', bulk*interleaved}
    }, ri, ms0, ms, bufs, ptrs}
  }
  
  each{{i, buf} => {
    ly{'bufsEnds'}{buf0,0}{'store', i, buf{'to_store',0}}
    @for_const(i to proc_bulk) buf{'store', i, exp_bulk} # allow reading a record at a past-the-end buf entry
  }, ri, bufs}
  
  def VI64 = [proc_bulk]i64;   def VU64 = ty_u{VI64}
  def VI32 = re_el{i32, VI64}
  def VI16 = re_el{i16, VI64}
  def VI8  = re_el{i8,  VI64}; def VU8  = ty_u{VI8}
  
  temp_buf:*i16 = object_arr{i16, vcount{VI16}**0}
  
  tail_mask:*i8 = static_arr{i8, merge{exp_bulk**0, exp_bulk ** -1}}
  def VX = [exp_bulk]i8
  def hash_vec{x:V & eltype{V}==i8} = {
    def HX = re_el{HASH, V}
    hv:= HX~~x
    hv^= l128_upper_u64{hv}
    hv^= hv >>{u64} 32
    hv^= hv >>{i32} 16
    # hv&= V~~re_el{i32,V}**255 # for testing hash collision behavior
  }
  
  ########## PROCESSING ##########
  @for(buf_i to interleaved) {
    def bufS = vptr{buf0, ly{'bufs'}{buf_i}}
    def bufE = vptr{buf0, ly{'bufsEnds'}{buf0,0}{'load', buf_i}}
    
  ########## PROCESSING ITERATION ##########
    retctr:ux = 0
    def buf_end = makelabel{}
    while (bufS{'lt', bufE}) { # proc_bulk records per iteration
      ########## NUMBER PARSING ##########
      # nums     off1     off2     aligned16
      # 01234567 01234567 01234567 01234567
      # 8.5↩ABC. 00000000 ..0.2.?? ..8.5.??
      # 17.1↩AB. 11111111 0.1.3.?? 1.7.1.??
      # 12.8↩AB. 11111111 0.1.3.?? 1.2.8.??
      # -10.1↩A. 22222222 1.2.4.?? 1.0.1.??
      #                   '.'=7    '.'=0
      
      idxs:= bufS{'load', VIX, 0}
      nums:= VI8~~gather{i64, inp, 0, idxs+VIX**1, 1} # each 8-byte group is a number
      
      # lps1{nums, 8}
      # display{'nums', nums}
      off:= VI8~~(v_eq{nums, VI8**46} >>{VI32} 23) &{VI64} VI64**3 # get offset of '.' minus 1 within i64 (i.e. '1.1'→0, '10.2'→1, '-1.2'→1, '-12.3'→2)
      # display{'off0', off}
      def low8_64 = l128shufb_ident{VI8}-cycle{vcount{VI8}, range{8}} # shuf nums for low byte within i64
      off = l128shufb{off, make{VI8, low8_64}} # expand the low byte of each i64 to each byte
      # display{'off1', off}
      
      nums = nums &{VI64} VI64**7r0xff # make last byte 0
      off|= make{VI8, cycle{vcount{VI8}, tup{0,7, 0,7, 0,7, 7,7}}}
      off+= make{VI8, cycle{vcount{VI8}, tup{-1,0, 0,0, 2,0, 0,0}} + low8_64}
      if (is_java) off&= VI8**l128shufb_mask{VI8}
      # display{'off2', off}
      aligned16:= VI16~~l128shufb{nums, off}
      
      # convert to digits, mapping '\n' and '.' to 0
      if (is_java) {
        aligned16 = __max{VI8}{aligned16 -{VI8} VI8**48, VI8**0} # no saturating ops
      } else {
        aligned16 = __subs{VU8}{aligned16, VU8**48}
      }
      
      aligned16*= make{VI16, cycle{vcount{VI16}, tup{100,10,1,0}}} # multiply by the place
      
      # sum the parts
      aligned16 = aligned16 + VI16~~((VU64~~aligned16)>>32)
      aligned16 = aligned16 + VI16~~((VU64~~aligned16)>>16)
      
      # negate negative numbers
      neg:= v_ne{VI16~~v_eq{nums, VI8**45}, VI16**0}
      aligned16^= ty_s{neg}
      aligned16-= ty_s{neg}
      # aligned16 is now [16]i16~~{num0, *, *, *, num1, *, *, *, num2, *, *, *, num3, *, *, *}
      store{temp_buf, 0, aligned16} # make it available for easy lookup in the loop
      
      # lpf1{'aligned16: ', aligned16}
      
      
      ########## HASHING ##########
      def left = bufE{'sub',bufS}
      
      # hashes two rows of names at a time on native; on Java that's slower :/
      def hashn = if (is_java) 1 else 2
      def VVX = [hashn * exp_bulk]i8
      
      def get_temp{ij} = load{temp_buf, ij*4}
      def to_fail{ij} = {
        # log fail, to be processed later (so codegen of the fast path isn't pessimized by the presence of a call here)
        failposC{'store', 0, bufS{'load', ij}}
        failposC{'store', 1, promote{vptr_store, get_temp{ij}}}
        failposC{'bump', 2}
      }
      
      @for (i to proc_bulk/hashn) {
        def ib = i*hashn
        def mask_newline{v:T} = if (width{T}==256) {
          def V64 = re_el{u64, T}
          a:= V64~~v_eq{v, T**10}
          l:= if (is_java) l128_upper_u64{a} & make{V64, cycle{vcount{V64}, tup{-1,0}}}
          else emit{V64, (if (width{V64}==256) '_mm256_bsrli_epi128' else '_mm_bsrli_si128'), a, 8}
          a = a | v_ne{l, V64**0}
          # a = a | a >> 8
          # a = a | a >> 16
          # a = a | a >> 32
          a = tree_fold{|, each{{c}=>a>>c, 4 + 16*range{4}}}
          v & T~~v_eq{T~~a, T**0}
        } else {
          mc:= homMask{v == VX**10}
          c:ux = exp_bulk
          if (likely{mc!=0}) c = clz{exp_bulk, mc}
          v & load{VX, tail_mask, c}
        }
        
        # load names
        def {parts, total} = {
          def for_vs{G} = @for_const(j to hashn) {
            off:= bufS{'load', ib + j}
            G{load{VX, inp, off-exp_bulk}}
          }
          if (is_java or hashn==1) {
            def parts = for_vs{mask_newline}
            tup{parts, pair{parts}}
          } else {
            def total = mask_newline{pair{for_vs{{x}=>x}}}
            tup{halves{total}, total}
          }
        }
        
        # hash
        def VHX = re_el{HASH, VVX}
        hv:VHX = hash_vec{total}
        
        def update{ij, idx} = {
          dataoff:= idx*4
          temp:= promote{i64, get_temp{ij}}
          def upd{k, G} = {
            def p = tup{map_data, dataoff+k}
            store{...p, G{load{...p}}}
          }
          upd{dt_min, {v} => __min{v, temp}}
          upd{dt_max, {v} => __max{v, temp}}
          upd{dt_sum, {v} => v + temp}
          upd{dt_num, {v} => v + 1}
        }
        
        if (hashn==2 and not is_java) {
          hx:= shuf{VHX, hv, 0,0,0,0, 4,4,4,4}
          idxv:= hv & VHX**hash_mask
          idxs:= each{{i} => promote{ux, extract{idxv,i}}, range{hashn} * 4}
          hashes:= pair{each{{idx} => load{HASHV, map_hash, idx}, idxs}}
          m:= homMask{hashes == hx}
          m2:= m & (m-1)
          if (rare{m2 == 0}) {
            def {i0,i1} = idxs
            def c = ctz{m}
            idxs = tup{i0+c, i1+c-vcount{HASHV}}
          } else {
            idxs = each{{idx,m} => idx+ctz{m}, idxs, tup{m,m2>>vcount{HASHV}}}
          }
          each{{j, idx} => {
            def ij = ib + j
            if (rare{ij >= left}) goto{buf_end}
            exp:= load{VX, map_exp, idx*exp_bulk}
            if (exp !== select{parts,j}) {
              to_fail{ij}
            } else {
              update{ij, idx}
            }
          }, range{hashn}, idxs}
        } else @for_const (j to hashn) {
          def ij = ib + j
          if (rare{ij >= left}) goto{buf_end}
          hash:= extract{hv, j*4}
          
          idx:= cast_i{ux, hash&hash_mask}
          m:= homMask{HASHV**hash == load{HASHV, map_hash, idx}}
          if (rare{m==0}) { # no matching hash value
            to_fail{ij}
          } else {
            idx+= ctz{m}
            exp:= load{VX, map_exp, idx*exp_bulk}
            if (exp !== select{parts,j}) {
              to_fail{ij}
            } else {
              update{ij, idx}
            }
          }
        }
      }
      
      if (one and ++retctr >= 3) return{}
      
      bufS{'bump', vcount{VIX}}
    }
    setlabel{buf_end}
  }
  
  # handle fails
  if (not playground) while (failposS{'lt', failposC}) {
    def end = failposS{'load', 0}
    def temp = ty_s{failposS{'load', 1}}
    failposS{'bump', 2}
    def case{which, start, ...hash} = {
      if (is_java) emit{void, merge{'main.Main.failed_',which}, ident, start, end, temp, ...hash}
      else         emit{void, merge{          'failed_',which},        start, end, temp, ...hash}
    }
    
    def v0 = load{VX, inp, end-exp_bulk}
    m:= homMask{v0 == VX**10}
    if (likely{m==0}) {
      start:= end
      do {
        start-= exp_bulk
        m = homMask{load{VX, inp, start-exp_bulk} == VX**10}
      } while (m==0)
      case{'long', start-clz{exp_bulk, m}}
    } else {
      c:= clz{exp_bulk, m}
      case{'short', end-c, extract{hash_vec{v0 & load{VX, tail_mask, c}},0}}
    }
  }
}

export{'core_1brc_buf_elts', core_1brc_buf_elts}
export{'core_1brc_periter', core_1brc_periter}
export{'core_1brc', core_1brc}

(if (is_java or playground) {
  fn minibench(ai8:*i8, ai16:*i16, ai32:*i32, ai64:*i64) : void = @withbufs(object_arr) {
    def VI8  = [16]i8;          def VU8  = ty_u{VI8}
    def VI16 = re_el{i16, VI8}; def VU16 = ty_u{VI16}
    def VI32 = re_el{i32, VI8}; def VU32 = ty_u{VI32}
    def VI64 = re_el{i64, VI8}; def VU64 = ty_u{VI64}
    @for (i to 1000) {
      def V = VI8
      def arr = match(elwidth{V}) { {(8)}=>ai8; {(16)}=>ai16; {(32)}=>ai32; {(64)}=>ai64 }
      
      def j = i * vcount{V}
      v:= load{V, arr, j}
      
      store{arr, j, V~~v}
    }
  }
  export{'minibench',minibench}
})

if (hasarch{'AVX2'}) {
  include 'clib/malloc'
  (if (playground) {
    require{'time.h'}
    fn nstime() = {
      buf:*u64 = 2**0
      emit{void, 'clock_gettime', 0, *void~~buf}
      load{buf,0}*1e9 + load{buf,1}
    }
  })
}